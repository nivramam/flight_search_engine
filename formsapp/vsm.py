# -*- coding: utf-8 -*-
"""vsm.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1XJyVGfZ24tOLeS6vMkDhGhgQ0xLucVi0
"""
import os
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def get_tokenized_list(doc_text):
    tokens = nltk.word_tokenize(doc_text)
    return tokens

def word_stemmer(token_list):
  ps = nltk.stem.PorterStemmer()
  stemmed = []
  for words in token_list:
    stemmed.append(ps.stem(words))
  return stemmed

def remove_stopwords(doc_text):
  cleaned_text = []
  for words in doc_text:
    if words not in stop_words:
      cleaned_text.append(words)
  return cleaned_text

#predefined queries
def queriesProcess():
    queries_file = open('D:\\Nivedhithaa\\Ninth Semester\\lab\\irrLab\\data\\queries.txt')
    queries = queries_file.readlines()
    print(len(queries))

    queries_proccessed = []
    for query in queries:
        tokens = get_tokenized_list(query)
        doc_text = remove_stopwords(tokens)
        doc_text = word_stemmer(doc_text)
        doc_text = ' '.join(doc_text)
        queries_proccessed.append(doc_text)
    print(len(queries_proccessed))

    query_vectors = []
    for query in queries_proccessed:
        query_vectors.append(vectorizerX.transform([query]))
    # print(len(query_vectors))

    for query_vector in query_vectors:
        cosineSimilarities = cosine_similarity(doc_vector, query_vector).flatten()
        related_docs_indices = cosineSimilarities.argsort()[:-10:-1]
        print(related_docs_indices)
        print(data_proccessed[related_docs_indices[0]])

#user-dynamic query
def handleQuery(query):
    queries_proccessed = []
    tokens = get_tokenized_list(query)
    q_text = remove_stopwords(tokens)
    q_text = word_stemmer(q_text)
    q_text = ' '.join(q_text)
    # queries_proccessed.append(q_text)
    # print(len(queries_proccessed))
    query_vector = vectorizerX.transform([query])
    cosineSimilarities = cosine_similarity(doc_vector, query_vector).flatten()
    related_docs_indices = cosineSimilarities.argsort()[:-10:-1]
    # print(data_proccessed[related_docs_indices[0]])
    your_string = data_proccessed[related_docs_indices[0]]
    removal_list = ["elevation560", "popul", "2011", "india"]

    edit_string_as_list = your_string.split()

    final_list = [word for word in edit_string_as_list if word not in removal_list]
    final_string = ' '.join(final_list)
    # print(final_string)
    return related_docs_indices
    # print(data_proccessed[related_docs_indices[0]])

def getDocsGivenIndices(indices_list):
    resultStr = []
    for i in indices_list:
        resultStr.append(indexDict.get('value')[i])
    print(resultStr)
    return resultStr

data = []
i = 0
indexDict = {'key' : [], 'value': []}
for filename in os.listdir('D:\\Nivedhithaa\\Ninth Semester\\lab\\irrLab\\data\\citiesData'):
    with open(os.path.join('D:\\Nivedhithaa\\Ninth Semester\\lab\\irrLab\\data\\citiesData\\', filename), 'r',
              encoding='utf-8') as f:
        # print(f.read())
        indexDict['key'].append(i)
        indexDict['value'].append(filename.replace("_wiki.txt", ""))
        i+=1
        data.append(f.read())
print(indexDict)
stop_words = set(stopwords.words('english'))
# print(stop_words)
data_proccessed = []
for line in data:
  tokens = get_tokenized_list(line)
  doc_text = remove_stopwords(tokens)
  doc_text  = word_stemmer(doc_text)
  doc_text = ' '.join(doc_text)
  data_proccessed.append(doc_text)
vectorizerX = TfidfVectorizer()
vectorizerX.fit(data_proccessed)

doc_vector = vectorizerX.transform(data_proccessed)
df1 = pd.DataFrame(doc_vector.toarray(), columns=vectorizerX.get_feature_names())
queriesProcess()
out = handleQuery("flights needed from bangalore to pune")
# print(type(out))
# getDocsGivenIndices([0,1])